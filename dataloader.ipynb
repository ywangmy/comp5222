{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Created on Fri Nov 24 2023 00:57:09\n",
    "# Author: Mukai (Tom Notch) Yu\n",
    "# Email: myual@connect.ust.hk\n",
    "# Affiliation: Hong Kong University of Science and Technology\n",
    "#\n",
    "# Copyright â’¸ 2023 Mukai (Tom Notch) Yu\n",
    "#\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.files import read_file, print_dict\n",
    "from dataloader.feature_extractor import FeatureExtractor\n",
    "from dataloader.perspective_warper import PerspectiveWarper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "  epochs: 10.0\n",
      "  learning_rate: 0.0001\n",
      "  dataset: \n",
      "    batch_size: 32.0\n",
      "    COCO: \n",
      "      path: ./COCO2014/train2014\n",
      "      fraction: 0.001\n",
      "      resize: [640.0, 480.0]\n",
      "      fix_warp: false\n",
      "      shuffle: true\n",
      "    ScanNet: \n",
      "      path: ./ScanNet/train\n",
      "      fraction: 0.001\n",
      "      resize: [640.0, 480.0]\n",
      "      fix_warp: false\n",
      "      shuffle: true\n",
      "eval: \n",
      "  output_dir: ./dump_match_pairs/\n",
      "  eval_interval: 5.0\n",
      "  dataset: \n",
      "    batch_size: 32.0\n",
      "    COCO: \n",
      "      path: ./COCO2014/eval2014\n",
      "      fraction: 0.001\n",
      "      resize: [640.0, 480.0]\n",
      "      fix_warp: true\n",
      "      shuffle: true\n",
      "    ScanNet: \n",
      "      path: ./ScanNet/eval\n",
      "      fraction: 0.001\n",
      "      resize: [640.0, 480.0]\n",
      "      fix_warp: true\n",
      "      shuffle: true\n",
      "superglue: \n",
      "  num_layers: 3.0\n",
      "  sinkhorn_iterations: 10.0\n",
      "  match_threshold: 0.2\n",
      "  descriptor_dim: 256.0\n",
      "feature_extraction: \n",
      "  max_keypoints: 64.0\n",
      "  descriptor_dim: 256.0\n",
      "  extractor: \n",
      "    superpoint: \n",
      "      keypoint_threshold: 0.005\n",
      "      nms_radius: 4.0\n",
      "      match_threshold: 0.2\n",
      "      remove_borders: 4.0\n",
      "      model_weight_path: ./models/weights/superpoint_v1.pth\n",
      "perspective_warper: \n",
      "  max_warp_match_pixel_distance: 10.0\n",
      "  homography: \n",
      "    perturbation_threshold: 0.2\n",
      "visualize: true # Visualize the matches and dump the plots\n",
      "visualize_output_dir: ./dump_match_pairs/ # Output directory for the visualization\n"
     ]
    }
   ],
   "source": [
    "config = read_file(\"./configs/default.yaml\")\n",
    "print_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_keypoints(image, keypoints, color=(0, 255, 0)):\n",
    "    # Create a copy of the input image to avoid modifying the original\n",
    "    image_with_keypoints = image.copy()\n",
    "\n",
    "    # Convert tensor keypoints to list of cv2.KeyPoint objects\n",
    "    cv_keypoints = [cv2.KeyPoint(x=kp[0], y=kp[1], size=10) for kp in keypoints.numpy()]\n",
    "\n",
    "    # Draw the keypoints on the copy of the image\n",
    "    image_with_keypoints = cv2.drawKeypoints(\n",
    "        image_with_keypoints,\n",
    "        cv_keypoints,\n",
    "        None,\n",
    "        color=color,\n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
    "    )\n",
    "\n",
    "    return image_with_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, config: dict, feature_extractor, perspective_warper):\n",
    "        self.config = config\n",
    "        self.path = config[\"path\"]\n",
    "        self.fraction = config[\"fraction\"]\n",
    "        self.resize = [int(dim) for dim in config[\"resize\"]]\n",
    "        self.fix_warp = config[\"fix_warp\"]\n",
    "        self.shuffle = config[\"shuffle\"]\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.perspective_warper = perspective_warper\n",
    "\n",
    "        self.files = []\n",
    "        # recursively walk though the directory\n",
    "        for root, _, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                    self.files.append(os.path.join(root, file))\n",
    "\n",
    "        # limit the number of files to self.fraction x total number of files\n",
    "        self.files = self.files[: int(len(self.files) * self.fraction)]\n",
    "\n",
    "        if self.fix_warp:\n",
    "            self.transforms = [\n",
    "                self.perspective_warper.generate_transform(*self.resize)\n",
    "                for _ in range(len(self.files))\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        file = self.files[index]\n",
    "        image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        if self.resize is not None:\n",
    "            image = cv2.resize(image, self.resize)\n",
    "\n",
    "        width, height = image.shape\n",
    "\n",
    "        # Retrieve or generate transform\n",
    "        transform = (\n",
    "            self.transforms[index]\n",
    "            if self.fix_warp\n",
    "            else self.perspective_warper.generate_transform(width, height)\n",
    "        )\n",
    "\n",
    "        # Apply transform\n",
    "        image_novel = self.perspective_warper.warp_image_to_novel(image, transform)\n",
    "\n",
    "        (\n",
    "            keypoints_original,\n",
    "            descriptors_original,\n",
    "            confidence_score_original,\n",
    "        ) = self.feature_extractor(image)\n",
    "        (\n",
    "            keypoints_novel,\n",
    "            descriptors_novel,\n",
    "            confidence_score_novel,\n",
    "        ) = self.feature_extractor(image_novel)\n",
    "\n",
    "        keypoints_original_warped_to_novel = (\n",
    "            self.perspective_warper.warp_keypoints_to_novel(\n",
    "                keypoints_original, transform\n",
    "            )\n",
    "        )\n",
    "        keypoints_novel_warped_to_original = (\n",
    "            self.perspective_warper.warp_keypoints_to_original(\n",
    "                keypoints_novel, transform\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Compute distances between warped original keypoints and keypoints from the novel image\n",
    "        dists_original_to_novel = torch.cdist(\n",
    "            keypoints_original_warped_to_novel, keypoints_novel\n",
    "        )\n",
    "        dists_novel_to_original = torch.cdist(\n",
    "            keypoints_original, keypoints_novel_warped_to_original\n",
    "        )\n",
    "\n",
    "        # run linear_sum_assignment to find mutual nearest neighbors\n",
    "        (\n",
    "            row_index_original_to_novel,\n",
    "            col_index_original_to_novel,\n",
    "        ) = linear_sum_assignment(dists_original_to_novel)\n",
    "        (\n",
    "            row_index_novel_to_original,\n",
    "            col_index_novel_to_original,\n",
    "        ) = linear_sum_assignment(dists_novel_to_original)\n",
    "\n",
    "        # Initialize binary matrices with zeros\n",
    "        binary_matrix_original_to_novel = np.zeros(dists_original_to_novel.shape)\n",
    "        binary_matrix_novel_to_original = np.zeros(dists_novel_to_original.shape)\n",
    "\n",
    "        # Fill in the binary matrices based on the Hungarian algorithm results and the distance threshold\n",
    "        for r, c in zip(row_index_original_to_novel, col_index_original_to_novel):\n",
    "            if (\n",
    "                dists_original_to_novel[r, c]\n",
    "                <= self.perspective_warper.max_warp_match_pixel_distance\n",
    "            ):\n",
    "                binary_matrix_original_to_novel[r, c] = 1\n",
    "        for r, c in zip(row_index_novel_to_original, col_index_novel_to_original):\n",
    "            if (\n",
    "                dists_novel_to_original[r, c]\n",
    "                <= self.perspective_warper.max_warp_match_pixel_distance\n",
    "            ):\n",
    "                binary_matrix_novel_to_original[r, c] = 1\n",
    "\n",
    "        # Perform the element-wise logical AND operation to find mutual matches\n",
    "        mutual_matches = np.logical_and(\n",
    "            binary_matrix_original_to_novel, binary_matrix_novel_to_original.T\n",
    "        )\n",
    "\n",
    "        # Initialize the partial assignment matrix with zeros (no matches)\n",
    "        max_kp = self.feature_extractor.max_keypoints\n",
    "        partial_assignment_matrix = np.zeros(\n",
    "            (max_kp + 1, max_kp + 1), dtype=np.float32\n",
    "        )  # +1 for dustbin\n",
    "\n",
    "        # Copy the assignment matrix into the upper left corner of the partial assignment matrix\n",
    "        partial_assignment_matrix[\n",
    "            : mutual_matches.shape[0], : mutual_matches.shape[1]\n",
    "        ] = mutual_matches\n",
    "\n",
    "        # Accommodate the dustbins\n",
    "        partial_assignment_matrix[max_kp, :max_kp] = 1 - np.sum(\n",
    "            partial_assignment_matrix[:max_kp, :max_kp], axis=0\n",
    "        )  # Dustbin for novel keypoints\n",
    "        partial_assignment_matrix[:max_kp, max_kp] = 1 - np.sum(\n",
    "            partial_assignment_matrix[:max_kp, :max_kp], axis=1\n",
    "        )  # Dustbin for original keypoint\n",
    "\n",
    "        # Convert the partial_assignment_matrix with dustbins to a PyTorch tensor\n",
    "        partial_assignment_matrix = torch.from_numpy(partial_assignment_matrix)\n",
    "\n",
    "        # Pad keypoints, descriptors, and confidence scores with zeros\n",
    "        padded_keypoints_original = torch.cat(\n",
    "            (\n",
    "                keypoints_original,\n",
    "                torch.zeros(\n",
    "                    (max_kp - keypoints_original.shape[0], keypoints_original.shape[1])\n",
    "                ),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        padded_keypoints_novel = torch.cat(\n",
    "            (\n",
    "                keypoints_novel,\n",
    "                torch.zeros(\n",
    "                    (max_kp - keypoints_novel.shape[0], keypoints_novel.shape[1])\n",
    "                ),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        padded_descriptors_original = torch.cat(\n",
    "            (\n",
    "                descriptors_original,\n",
    "                torch.zeros(\n",
    "                    (max_kp - descriptors_novel.shape[0], descriptors_novel.shape[1])\n",
    "                ),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        padded_descriptors_novel = torch.cat(\n",
    "            (\n",
    "                descriptors_novel,\n",
    "                torch.zeros(\n",
    "                    (max_kp - descriptors_novel.shape[0], descriptors_novel.shape[1])\n",
    "                ),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        padded_confidence_scores_original = torch.cat(\n",
    "            (\n",
    "                confidence_score_original,\n",
    "                torch.zeros((max_kp - confidence_score_original.shape[0])),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        padded_confidence_scores_novel = torch.cat(\n",
    "            (\n",
    "                confidence_score_novel,\n",
    "                torch.zeros((max_kp - confidence_score_novel.shape[0])),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image_original\": image,\n",
    "            \"image_novel\": image_novel,\n",
    "            \"keypoints_original\": padded_keypoints_original,\n",
    "            \"keypoints_novel\": padded_keypoints_novel,\n",
    "            \"descriptors_original\": padded_descriptors_original,\n",
    "            \"descriptors_novel\": padded_descriptors_novel,\n",
    "            \"confidence_scores_original\": padded_confidence_scores_original,\n",
    "            \"confidence_scores_novel\": padded_confidence_scores_novel,\n",
    "            \"partial_assignment_matrix\": partial_assignment_matrix,\n",
    "        }\n",
    "\n",
    "        keypoints_original_visualization = visualize_keypoints(\n",
    "            image, keypoints_original\n",
    "        )\n",
    "        plt.imshow(keypoints_original_visualization)\n",
    "        plt.show()\n",
    "\n",
    "        keypoints_original_warped_to_novel_visualization = visualize_keypoints(\n",
    "            image_novel, keypoints_original_warped_to_novel\n",
    "        )\n",
    "        plt.imshow(keypoints_original_warped_to_novel_visualization)\n",
    "        plt.show()\n",
    "\n",
    "        keypoints_novel_visualization = visualize_keypoints(\n",
    "            image_novel, keypoints_novel\n",
    "        )\n",
    "        plt.imshow(keypoints_novel_visualization)\n",
    "        plt.show()\n",
    "\n",
    "        keypoints_novel_warped_to_original_visualization = visualize_keypoints(\n",
    "            image, keypoints_novel_warped_to_original\n",
    "        )\n",
    "        plt.imshow(keypoints_novel_warped_to_original_visualization)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_original': array([[ 81,  84,  80, ..., 190, 203, 213],\n",
       "        [ 93,  88,  82, ..., 145, 149, 152],\n",
       "        [103,  89,  85, ..., 107, 104, 103],\n",
       "        ...,\n",
       "        [ 46,  45,  45, ...,  35,  35,  34],\n",
       "        [ 38,  38,  40, ...,  32,  33,  33],\n",
       "        [ 31,  32,  34, ...,  31,  32,  33]], dtype=uint8),\n",
       " 'image_novel': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'keypoints_original': tensor([[234., 265.],\n",
       "         [564.,  47.],\n",
       "         [449., 255.],\n",
       "         [406., 270.],\n",
       "         [255., 213.],\n",
       "         [461., 288.],\n",
       "         [225., 257.],\n",
       "         [224.,  61.],\n",
       "         [449., 271.],\n",
       "         [407., 234.],\n",
       "         [557., 459.],\n",
       "         [435., 271.],\n",
       "         [393., 207.],\n",
       "         [268., 333.],\n",
       "         [200., 317.],\n",
       "         [254., 333.],\n",
       "         [435., 237.],\n",
       "         [493., 258.],\n",
       "         [245., 171.],\n",
       "         [243., 257.],\n",
       "         [350., 266.],\n",
       "         [281., 319.],\n",
       "         [462., 256.],\n",
       "         [243., 240.],\n",
       "         [449., 239.],\n",
       "         [581.,  24.],\n",
       "         [191., 230.],\n",
       "         [407., 221.],\n",
       "         [212., 332.],\n",
       "         [449., 227.],\n",
       "         [364., 318.],\n",
       "         [462., 272.],\n",
       "         [422., 236.],\n",
       "         [430., 201.],\n",
       "         [381.,  56.],\n",
       "         [605., 455.],\n",
       "         [101., 240.],\n",
       "         [402., 129.],\n",
       "         [324., 173.],\n",
       "         [205., 256.],\n",
       "         [268., 319.],\n",
       "         [362., 246.],\n",
       "         [479., 231.],\n",
       "         [ 37.,  57.],\n",
       "         [198., 332.],\n",
       "         [174., 211.],\n",
       "         [117., 253.],\n",
       "         [492., 273.],\n",
       "         [436., 225.],\n",
       "         [187., 317.],\n",
       "         [268., 243.],\n",
       "         [246., 274.],\n",
       "         [359., 302.],\n",
       "         [457., 207.],\n",
       "         [117., 238.],\n",
       "         [620.,  26.],\n",
       "         [477., 272.],\n",
       "         [594.,  98.],\n",
       "         [324., 192.],\n",
       "         [341., 291.],\n",
       "         [361., 197.],\n",
       "         [479., 211.],\n",
       "         [242.,  35.],\n",
       "         [338.,  35.]]),\n",
       " 'keypoints_novel': tensor([[256., 373.],\n",
       "         [232., 312.],\n",
       "         [199., 279.],\n",
       "         [209., 302.],\n",
       "         [241., 292.],\n",
       "         [202., 352.],\n",
       "         [101., 447.],\n",
       "         [387., 318.],\n",
       "         [244., 371.],\n",
       "         [241., 321.],\n",
       "         [431., 310.],\n",
       "         [433., 299.],\n",
       "         [381., 128.],\n",
       "         [389., 301.],\n",
       "         [385., 334.],\n",
       "         [343., 327.],\n",
       "         [429., 325.],\n",
       "         [476., 318.],\n",
       "         [240., 306.],\n",
       "         [161., 220.],\n",
       "         [554., 178.],\n",
       "         [332., 325.],\n",
       "         [443., 281.],\n",
       "         [587., 118.],\n",
       "         [268., 363.],\n",
       "         [472., 347.],\n",
       "         [395., 201.],\n",
       "         [446., 301.],\n",
       "         [192., 351.],\n",
       "         [461., 315.],\n",
       "         [413., 339.],\n",
       "         [458., 331.],\n",
       "         [475., 333.],\n",
       "         [345., 308.],\n",
       "         [235., 132.],\n",
       "         [462., 229.],\n",
       "         [252., 270.],\n",
       "         [173., 317.],\n",
       "         [252., 107.],\n",
       "         [627., 221.],\n",
       "         [130., 281.],\n",
       "         [299., 194.],\n",
       "         [200., 364.],\n",
       "         [474., 119.],\n",
       "         [153., 292.],\n",
       "         [468., 362.],\n",
       "         [314., 255.],\n",
       "         [372., 333.],\n",
       "         [438., 358.],\n",
       "         [257., 361.],\n",
       "         [211., 365.],\n",
       "         [266., 136.],\n",
       "         [258., 329.],\n",
       "         [399., 336.],\n",
       "         [390., 289.],\n",
       "         [154., 276.],\n",
       "         [142., 280.],\n",
       "         [142., 136.],\n",
       "         [415., 323.],\n",
       "         [319., 220.],\n",
       "         [327., 372.],\n",
       "         [340., 107.],\n",
       "         [229., 381.],\n",
       "         [206., 321.]]),\n",
       " 'descriptors_original': tensor([[-0.0380,  0.0480, -0.0925,  ..., -0.0534, -0.0062,  0.1208],\n",
       "         [-0.0121,  0.0352, -0.1148,  ...,  0.0059,  0.0080,  0.0636],\n",
       "         [-0.0101, -0.1507,  0.0098,  ...,  0.0711, -0.0117, -0.0539],\n",
       "         ...,\n",
       "         [ 0.0802, -0.0519, -0.0130,  ..., -0.1286, -0.0283,  0.0574],\n",
       "         [ 0.0073, -0.0358,  0.0136,  ...,  0.0935,  0.0132, -0.0177],\n",
       "         [-0.0371, -0.0180, -0.0471,  ..., -0.1027, -0.0568,  0.0225]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 'descriptors_novel': tensor([[ 0.0094, -0.0129,  0.1577,  ...,  0.0274, -0.0808,  0.0615],\n",
       "         [-0.0431,  0.0582, -0.0892,  ..., -0.0739, -0.0243,  0.1373],\n",
       "         [ 0.0157, -0.0667, -0.0584,  ...,  0.1158,  0.0791, -0.0262],\n",
       "         ...,\n",
       "         [-0.0430, -0.0155, -0.0710,  ..., -0.1148, -0.0820,  0.0341],\n",
       "         [-0.0967,  0.0632,  0.0863,  ...,  0.0025,  0.0442,  0.0049],\n",
       "         [ 0.0095, -0.0724,  0.0241,  ...,  0.0269,  0.0980,  0.0044]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 'confidence_scores_original': tensor([[0.6562],\n",
       "         [0.6151],\n",
       "         [0.5916],\n",
       "         [0.5745],\n",
       "         [0.5665],\n",
       "         [0.5635],\n",
       "         [0.5467],\n",
       "         [0.5429],\n",
       "         [0.5414],\n",
       "         [0.5387],\n",
       "         [0.5273],\n",
       "         [0.5087],\n",
       "         [0.5059],\n",
       "         [0.5012],\n",
       "         [0.4907],\n",
       "         [0.4850],\n",
       "         [0.4826],\n",
       "         [0.4817],\n",
       "         [0.4804],\n",
       "         [0.4738],\n",
       "         [0.4712],\n",
       "         [0.4710],\n",
       "         [0.4682],\n",
       "         [0.4608],\n",
       "         [0.4542],\n",
       "         [0.4530],\n",
       "         [0.4524],\n",
       "         [0.4484],\n",
       "         [0.4429],\n",
       "         [0.4417],\n",
       "         [0.4355],\n",
       "         [0.4344],\n",
       "         [0.4322],\n",
       "         [0.4277],\n",
       "         [0.4164],\n",
       "         [0.4083],\n",
       "         [0.4077],\n",
       "         [0.4038],\n",
       "         [0.4004],\n",
       "         [0.3982],\n",
       "         [0.3936],\n",
       "         [0.3877],\n",
       "         [0.3838],\n",
       "         [0.3834],\n",
       "         [0.3772],\n",
       "         [0.3748],\n",
       "         [0.3735],\n",
       "         [0.3724],\n",
       "         [0.3694],\n",
       "         [0.3659],\n",
       "         [0.3655],\n",
       "         [0.3647],\n",
       "         [0.3629],\n",
       "         [0.3622],\n",
       "         [0.3596],\n",
       "         [0.3581],\n",
       "         [0.3575],\n",
       "         [0.3559],\n",
       "         [0.3555],\n",
       "         [0.3462],\n",
       "         [0.3454],\n",
       "         [0.3452],\n",
       "         [0.3449],\n",
       "         [0.3423]], grad_fn=<CatBackward0>),\n",
       " 'confidence_scores_novel': tensor([[0.6782],\n",
       "         [0.6729],\n",
       "         [0.6572],\n",
       "         [0.5830],\n",
       "         [0.5278],\n",
       "         [0.5199],\n",
       "         [0.5195],\n",
       "         [0.5195],\n",
       "         [0.4883],\n",
       "         [0.4857],\n",
       "         [0.4826],\n",
       "         [0.4817],\n",
       "         [0.4802],\n",
       "         [0.4797],\n",
       "         [0.4722],\n",
       "         [0.4680],\n",
       "         [0.4646],\n",
       "         [0.4553],\n",
       "         [0.4425],\n",
       "         [0.4420],\n",
       "         [0.4357],\n",
       "         [0.4264],\n",
       "         [0.4255],\n",
       "         [0.4180],\n",
       "         [0.4153],\n",
       "         [0.4141],\n",
       "         [0.4040],\n",
       "         [0.3996],\n",
       "         [0.3948],\n",
       "         [0.3870],\n",
       "         [0.3863],\n",
       "         [0.3856],\n",
       "         [0.3831],\n",
       "         [0.3820],\n",
       "         [0.3820],\n",
       "         [0.3818],\n",
       "         [0.3806],\n",
       "         [0.3799],\n",
       "         [0.3793],\n",
       "         [0.3790],\n",
       "         [0.3786],\n",
       "         [0.3785],\n",
       "         [0.3783],\n",
       "         [0.3763],\n",
       "         [0.3739],\n",
       "         [0.3714],\n",
       "         [0.3710],\n",
       "         [0.3707],\n",
       "         [0.3691],\n",
       "         [0.3690],\n",
       "         [0.3667],\n",
       "         [0.3631],\n",
       "         [0.3627],\n",
       "         [0.3622],\n",
       "         [0.3617],\n",
       "         [0.3596],\n",
       "         [0.3581],\n",
       "         [0.3559],\n",
       "         [0.3469],\n",
       "         [0.3453],\n",
       "         [0.3445],\n",
       "         [0.3437],\n",
       "         [0.3432],\n",
       "         [0.3427]], grad_fn=<CatBackward0>),\n",
       " 'partial_assignment_matrix': tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 0.]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(config[\"feature_extraction\"])\n",
    "perspective_warper = PerspectiveWarper(config[\"perspective_warper\"])\n",
    "\n",
    "coco_training_dataset = COCODataset(\n",
    "    config[\"train\"][\"dataset\"][\"COCO\"], feature_extractor, perspective_warper\n",
    ")\n",
    "coco_training_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-feature-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
